{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d7beae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 903a357] Attempts to fix 3d issue with a reshape\n",
      " 1 file changed, 40 insertions(+), 32 deletions(-)\n",
      "Enumerating objects: 5, done.\n",
      "Counting objects: 100% (5/5), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 1022 bytes | 511.00 KiB/s, done.\n",
      "Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/RudimentaryChef/FissionTrack.git\n",
      "   2d30611..903a357  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"Attempts to fix 3d issue with a reshape\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a423767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required libraries\n",
    "#SAM\n",
    "#!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
    "#Transformers\n",
    "#!pip install -q git+https://github.com/huggingface/transformers.git\n",
    "#Datasets to prepare data and monai if you want to use special loss functions\n",
    "#!pip install datasets\n",
    "#!pip install -q monai\n",
    "#Patchify to divide large images into smaller patches for training. (Not necessary for smaller images)\n",
    "#!pip install patchify\n",
    "\n",
    "#!pip install matplotlib\n",
    "\n",
    "#!pip install tifffile\n",
    "\n",
    "#!pip install scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e52272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numpy for handling data in vectors, matplotlib to graph\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#Library to work with TIFFs\n",
    "import tifffile\n",
    "\n",
    "#Importing OS for \n",
    "import os\n",
    "#We are going to work with large images and patchify helps with dividing it up\n",
    "import patchify as p\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "#Random imported for sampling\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e943e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def read_image_and_mask(image_path, mask_path):\n",
    "    \"\"\"\n",
    "    Reads an image and a mask from the specified paths and returns them as NumPy arrays.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        mask_path (str): Path to the mask file.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the image array and the mask array.\n",
    "    \"\"\"\n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Reshape the grayscale image to have three channels by adding an additional dimension\n",
    "    three_channel_image = np.expand_dims(gray_image, axis=-1)\n",
    "\n",
    "    # Repeat the single channel across three channels to convert it to RGB format\n",
    "    rgb_image = np.repeat(three_channel_image, 3, axis=-1)\n",
    "\n",
    "    # Read the mask image using OpenCV\n",
    "    mask_image = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Convert the images to NumPy arrays\n",
    "    image_array = np.array(rgb_image)\n",
    "    mask_array = np.array(mask_image)\n",
    "    \n",
    "    return image_array, mask_array\n",
    "\n",
    "# Example usage:\n",
    "image_array, mask_array = read_image_and_mask('/Users/adikrish/Desktop/Adam.jpg', '/Users/adikrish/Desktop/Eve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cea9a1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1504, 2048)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_array.shape\n",
    "\n",
    "mask_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16716074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a step size and a patch size\n",
    "patch_size = 256 \n",
    "step = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebe07b95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`window_shape` is incompatible with `arr_in.shape`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m all_mask_patches \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m large_mask \u001b[38;5;241m=\u001b[39m mask_array  \u001b[38;5;66;03m# Assuming large_masks is a single image of size a x b\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m patches_mask \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mpatchify(large_mask, (patch_size, patch_size, \u001b[38;5;241m3\u001b[39m), step\u001b[38;5;241m=\u001b[39mstep)  \u001b[38;5;66;03m# Step=256 for 256 patches means no overlap\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(patches_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(patches_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/patchify/__init__.py:32\u001b[0m, in \u001b[0;36mpatchify\u001b[0;34m(image, patch_size, step)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpatchify\u001b[39m(image: np\u001b[38;5;241m.\u001b[39mndarray, patch_size: Imsize, step: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    Split a 2D or 3D image into small patches given the patch size.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    >>> assert (reconstructed_image == image).all()\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m view_as_windows(image, patch_size, step)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/patchify/view_as_windows.py:28\u001b[0m, in \u001b[0;36mview_as_windows\u001b[0;34m(arr_in, window_shape, step)\u001b[0m\n\u001b[1;32m     26\u001b[0m     window_shape \u001b[38;5;241m=\u001b[39m (window_shape,) \u001b[38;5;241m*\u001b[39m ndim\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(window_shape) \u001b[38;5;241m==\u001b[39m ndim):\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`window_shape` is incompatible with `arr_in.shape`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(step, numbers\u001b[38;5;241m.\u001b[39mNumber):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: `window_shape` is incompatible with `arr_in.shape`"
     ]
    }
   ],
   "source": [
    "all_img_patches = []\n",
    "\n",
    "large_image = image_array # Assuming large_images is a single image of size a x b\n",
    "patches_img = p.patchify(large_image, (patch_size, patch_size), step=step)  # Step=256 for 256 patches means no overlap\n",
    "\n",
    "for i in range(patches_img.shape[0]):\n",
    "    for j in range(patches_img.shape[1]):\n",
    "        single_patch_img = patches_img[i,j,:,:]\n",
    "        all_img_patches.append(single_patch_img)\n",
    "\n",
    "images = np.array(all_img_patches)\n",
    "\n",
    "# Let us do the same for masks\n",
    "all_mask_patches = []\n",
    "\n",
    "large_mask = mask_array  # Assuming large_masks is a single image of size a x b\n",
    "patches_mask = p.patchify(large_mask, (patch_size, patch_size), step=step)  # Step=256 for 256 patches means no overlap\n",
    "\n",
    "for i in range(patches_mask.shape[0]):\n",
    "    for j in range(patches_mask.shape[1]):\n",
    "        single_patch_mask = patches_mask[i,j,:,:]\n",
    "        single_patch_mask = (single_patch_mask / 255.).astype(np.uint8)\n",
    "        all_mask_patches.append(single_patch_mask)\n",
    "\n",
    "masks = np.array(all_mask_patches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624bb103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we filter out of our masks any fully zero patches\n",
    "valid_indices = [i for i, masks in enumerate(masks) if masks.max() != 0]\n",
    "filtered_images = images[valid_indices]\n",
    "filtered_masks = masks[valid_indices]\n",
    "print(\"Image shape:\", filtered_images.shape)\n",
    "print(\"mask shape: \", filtered_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f9a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#move it to a dataset object\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "dataset_dict = {\n",
    "    'image': [Image.fromarray(img) for img in filtered_images],\n",
    "    \"label\": [Image.fromarray(mask) for mask in filtered_masks]\n",
    "}\n",
    "dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0d04d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We only have 40 images because we only have 40 masks\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce229f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A quick sanity check to see if our masks are working properly\n",
    "img_num = random.randint(0, filtered_images.shape[0]-1)\n",
    "example_image = dataset[img_num][\"image\"]\n",
    "example_mask = dataset[img_num][\"label\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the first image on the left\n",
    "axes[0].imshow(np.array(example_image), cmap='gray')  # Assuming the first image is grayscale\n",
    "axes[0].set_title(\"Image\")\n",
    "\n",
    "# Plot the second image on the right\n",
    "axes[1].imshow(example_mask, cmap='gray')  # Assuming the second image is grayscale\n",
    "axes[1].set_title(\"Mask\")\n",
    "\n",
    "# Hide axis ticks and labels\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "# Display the images side by side\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c6c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets a bounding box and applies it to our image\n",
    "#copied directly from reference\n",
    "def get_bounding_box(ground_truth_map):\n",
    "    y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "    #adds a little tolerance \n",
    "    H, W = ground_truth_map.shape\n",
    "    x_min = max(0,x_min - np.random.randint(0,20))\n",
    "    x_max = min(W, x_max + np.random.randint(0,20))\n",
    "    y_min = max(0,y_min - np.random.randint(0,20))\n",
    "    y_max = min(H, y_max + np.random.randint(0,20))\n",
    "    bounding_box = [x_min,y_min,x_max,y_max]\n",
    "    return bounding_box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SAMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This class creates a dataset with input images and masks.\n",
    "    Overrides the __len__ and __getitem__ methods of the Dataset class\n",
    "    \"\"\"\n",
    "    #constructor that provides the data set and the processor\n",
    "    def __init__(self,dataset,processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #gives us an image and the groundtruth box\n",
    "        item = self.dataset[idx]\n",
    "        image = item[\"image\"]\n",
    "        ground_truth_mask = np.array(item[\"label\"])\n",
    "        #gives us the bounding box for the image\n",
    "        prompt = get_bounding_box(ground_truth_mask)\n",
    "        # prepare iamge and prompt for the model\n",
    "        inputs = self.processor(image,input_boxes = [[prompt]], return_tensors = \"pt\")\n",
    "        #removes the batch dimension (which is added by default by the processor need to look into this more)\n",
    "        inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "        #Adds ground truth segmentation\n",
    "        inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964c57be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initalizes our processor\n",
    "from transformers import SamProcessor\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a30967",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SAMDataset(dataset,processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da0619",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3dd9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are customizing our data loader\n",
    "#Put drop_last to True if you get some tensor error\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset,batch_size = 1, shuffle = True, drop_last = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26756ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming batch is a dictionary containing image data\n",
    "#Use batch = next(iter(train_dataloader)) for 3d Array\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14482200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c734d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "#makes sure we only compute the gradients for what we need\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "        param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aead3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
